---
title: "hddglm"
subtitle: "High-Dimensional Dynamic Generalized Linear Models"
author: Maarja Osi-Baumeister
toc: true
format: 
  html: default
editor: visual
---

This document contains a guide on the R-package `hddglm` developed as part of the master thesis *High-Dimensional Dynamic Generalized Linear Models: A Bayesian Approach*. We demonstrate the steps to that were also used for simulation study from the thesis (Chapter 4).

This document was created using quarto.

## Setup

### Installing the package hddglm

As the first step, download the binary file `hddglm.zip` from [https://github.com/maarjaosi/hddglm/blob/main/hddglm.zip](https://github.com/maarjaosi/hdDglm/blob/main/hdDglm.zip){.uri}. Then install the package by calling the function `install.packages()` with the path to the downloaded file. In the example below the file is in the Download order. **The path needs to be changed to the file location on your computer.**

```{r}
#| eval: false
install.packages("~/Downloads/hddglm.zip")
```

When the installation is done you can load the package. We also load the package `MASS` to sample from the multivariate normal distribution when creating the artificial data.

```{r}
#| eval: false
library(hddglm)
library(MASS)

```

## Artificial Data

In this section, we create the artificial data set for $p = 10$ from Section 4.1 in the thesis.

## Setting parameters

We set the parameters to create an artificial data set.

```{r}
#| eval: false
#percentage of parameters that are non-zero
s <- 0.1 
#number of trials of the binomial distrbution
n <- 1 
#number of covariates
p <- 10 
#number of time steps
T <- 100 
#error covariance
W <- 0.0001*diag(p) 

#set a seed for the random number generator
set.seed(2023)
```

### Covariates

Each covariate $x_{ti}$ is sampled from the $\mathcal{N}(0,1)$ distribution and saved as the $(t,i)$-element of a $T \times p$-matrix.

```{r}
#| eval: false
#create the covariates 
x <- matrix(rnorm(T*p, mean = 0, sd = 1),
            nrow = T,
            ncol = p)
```

### Regression parameters

The regression parameters at time $t = 0$ are created by randomly choosing a set of relevant parameters `relevant_param_0`. For parameters $i$ in the set `relevant_param_0`, the value of $\beta_{0i}$ is sampled from the $\mathcal{N}(2,1)$-distribution else this value is let at zero.

```{r}
#| eval: false
#initialize beta
beta <- matrix(0, nrow = T + 1, ncol = p)
    
#randomly draw the relevant parameters for t = 0
nr_relevant_param <- floor(p*s)
relevant_param_0 <- sample(1:p, nr_relevant_param)
relevant_param <- relevant_param_0

#draw the values 
beta[1, relevant_param_0] <-  rnorm(nr_relevant_param, mean = 2, sd = 1)

```

Regression parameters at time steps $t \in \{1,...,100\}\setminus \{25,75\}$ are created by adding noise to the regression parameters at previous time steps. At time steps $t \in \{25, 75\}$ we change the set of relevant parameters by dropping one variable and adding a new one. The value of the new variable will be sampled form $\mathcal{N}(2,1)$ and the value of the old variable will be set to zero. The relevant parameters at time steps $t \in \{25, 75\}$ will be saved in the vectors `relevant_param_25` and `relevant_param_75` respectively.

```{r}
#| eval: false

for (t in 1:T) {
  #change relevant parameters
  if (t %in% c(25, 75)) {
        #which parameter should be left out; for only one relevant parameter
        #we just remove it since sampel() has a different behaviour for n = 1
          if (length(relevant_param_0) == 1) {
            old_parameter <- relevant_param 
          }
          else {
            old_parameter <- sample(relevant_param,1)
          }
          
          new_parameter <- sample((1:p)[-relevant_param],1)
          
          new_param_values <- beta[t, ]
          new_param_values[old_parameter] <- 0
          new_param_values[new_parameter] <- rnorm(1, mean = 2, sd = 1)
          #which parameters are relevant now
          
          relevant_param <- c(relevant_param[relevant_param != old_parameter],
                              new_parameter) 
          assign(paste("relevant_param", t, sep = "_"),
                 relevant_param,
                 envir = .GlobalEnv)
          
          beta[t + 1, ] <- new_param_values + MASS::mvrnorm(1, mu = rep(0, p), Sigma = W)  
  }
  else {
          beta[t + 1, ] <- beta[t, ] + MASS::mvrnorm(1, mu = rep(0, p), Sigma = W)  
  }
}
```

### Observations

Each observation $y_t$ is sampled from $\text{Bin}(1, q_t)$ where $q_t = \frac{\exp(\eta_t)}{1 + \exp(\eta_t)}$ for $\eta_t = \boldsymbol{x}_t^T \boldsymbol{\beta}_t$

```{r}
#| eval: false
#create the binomial data 
y <- rep(0,T)
for (t in 1:T) {
  eta <- x[t, ] %*% beta[t + 1, ]
  q <- exp(eta)/(1 + exp(eta))
  y[t] <- rbinom(1, n, prob = q)
}
```

## 

## Gibbs sampler

To run the Gibbs sampler based on data augmentation (see Algorithm 2.2.10 in the thesis), we first initialize the prior values. See also Section 4.1, *Model and prior values* in the thesis. We also define the evolution matrices as an R-function with time $t$ as an argument.

```{r}
#| eval: false

#initialize values
#prior mean and covariacne of the regression parameters
m0 <- rep(0,p)
C0 <- diag(1,p)
    
#prior values for the parameters of the distribution of the diagonal elements of the inverse gamma distribution
alpha <- rep(100, p)
gamma <- rep(1, p)
    
#prior values for the latent variables
omega <- rep(1/100, T)

#number of iterations of the Gibbs sampler
K <- 5000

#Evolution matrix
G <- function(t) {
  G_t <- diag(p)
  return(G_t)
}
```

We run the Gibbs sampler for the dynamic binary logistic model by calling the function `binomGibbSampler()`. We also record the time it took for the function to run.

```{r}
#| eval: false

start_time <- Sys.time()
beta_chain <- hddglm::binomDglmGibbs(model = "binom",
                             K  = K,
                             y = y,
                             x = x,
                             G = G,
                             m0  = m0,
                             C0 = C0,
                             n = n,
                             omega = omega,
                             alpha = alpha,
                             gamma = gamma)$beta_chain
end_time <- Sys.time()
elapsed_time <- end_time - start_time
```

### ESS

To calculate the median ESS, we apply the function `msESS` on the produced chain of parameter values $\{\boldsymbol{\beta}_{ti}^{(k)}\}_{k = 500}^{K}$ for each $t = 1,...,T$ and $i = 1,...,p$ save these into a $T \times p$ -matrix.This function calculates the *monotone sequence estimator* of ESS. See also Section 4.2 in the thesis for further explanation. Finally, we calculate the median over all ESS values.

```{r}
#| eval: false
 ESS_mat <- matrix(0, nrow = T, ncol = p)
  for (t in 1:T) {
      for (i in 1:p) {
        #get the chain for beta_{ti}
        index_covariate <- seq(i, (K-1)*p + i , p)
        beta_ti <- beta_chain[t, index_covariate][500:K]
        ESS_mat[t,i] <- hddglm::msESS(beta_ti)
    }
  }
    
ESS_median <- median(ESS_mat)
```

### Estimating moments

To estimate moments $E(\boldsymbol{\beta}_T \: \vert \: \boldsymbol{y}_{1:T})$ and $\text{Cov}(\boldsymbol{\beta}_T \: \vert \: \boldsymbol{y}_{1:T})$ , we apply the function `estimateMoments.`

```{r}
#| eval: false

# Estimate the moments of the regression parameters.
moments_mcmc <- hddglm::estimateMoments(beta_chain, p = p, t = T, K = K)
mean_mcmc <- moments_mcmc$mean
cov_mcmc <- moments_mcmc$cov
    
```

## Linear Bayes Method

We also apply the Linear Bayes method for the dynamic binary logistic model to calculate the estimates for the moments by calling the function `logisticLinearBayes`.

```{r}
#| eval: false
#calculate linear bayes estimates
moments_lb <- hdDglm::logisticLinearBayes(y = y,
                                          x = x,
                                          m0 = m0,
                                          C0 = C0,
                                          G = G,
                                          W = W)

mean_lb <- as.vector(moments_lb$m.list[[T]])
cov_lb <- moments_lb$C.list[[T]]


    
```

## Penalized Credible Regions Method

To produce a model sequence / solution path at time step $T$ we apply the function `pcr`. The values of the coefficients for the different models are obtained by solving the corresponding optimization problem.

The model sequence is a $p\times p$ matrix, whose element $(i,j)$ is `TRUE` when the $j-$ th variable is included into the $i$-th model in the sequence otherwise it is `FALSE`.

```{r}
#| eval: false
#model sequence based on the credible regions method

#based on the moments obtained from the samples produced by the Gibbs sampler
pcr_result_mcmc <- hddglm::pcr(mean_mcmc, cov_mcmc)
solution_path_mcmc <- pcr_result_mcmc$solution_path
coef_values_mcmc <- pcr_result_mcmc$coef_values

#Based on the moments calculated by the Linear Bayes method
pcr_result_lb <- hddglm::pcr(mean_lb, cov_lb)
solution_path_lb <- pcr_result_lb$solution_path
coef_values_lb <- pcr_result_lb$coef_values


```

### Variable ranking

Please also see Section 4.3 in the thesis. We evaluate the model sequences by plotting the ROC-Curves (see Figures 4.2 - 4.4 in the thesis) and calculating their $\text{AUC}$ values by applying the function `rocPlot` on the model sequences. This function returns a named list which contains the plot and the $\text{AUC}$ value.

```{r}
#| eval: false
#The real set of relevant parameters at time step T = 100 is the same 
#as at time step 75
true_model <- 1:p %in% relevant_param_75
    
    
#roc-curve and auc for MCMC
rocObj_mcmc <- hddglm::rocPlot(solution_path_mcmc,
                              true_model = true_model,
                              "MCMC")
rocObj_mcmc$plot

AUC_mcmc <- rocObj_mcmc$AUC

#roc-curve and auc for Linear Bayes
rocObj_lb <- hddglm::rocPlot(solution_path_lb,
               true_model = true_model,
               "Linear Bayes")
rocObj_lb$plot

AUC_lb <- rocObj_lb$AUC
    
```

To create a variable ranking table that shows which variables are included into which models in the sequence (see Tables 4.2 - 4.3 in the thesis) in the thesis, we call the function `variableRanking`on the solution paths.

```{r}
#| eval: false
hddglm::variableRanking(solution_path_mcmc)

hddglm::variableRanking(solution_path_lb)
```

### Variable selection

To select the final set of variables, we apply $\text{AIC}$ on the model sequences obtained based on the moments estimated with help of the Gibbs sampler and Linear Bayes method by calling the function `aicModel`.

```{r}
#| eval: false

#results based on MCMC algorithm  
aic_model_mcmc <- hddglm::aicModel(solution_path_mcmc,
                                  coef_values_mcmc,
                                  mean_mcmc,
                                  x[T,],
                                  y[T],
                                  T)
#results based on the Linear Bayes method
aic_model_lb <- hddglm::aicModel(solution_path_lb,
                                coef_values_lb,
                                mean_lb,
                                x[T,],
                                y[T],
                                T)

```

For the model sequence obtained based the moments estimated with the help of the MCMC algorithm, we also apply $\text{DIC}$ by calling the function `dicModel`.

```{r}
#| eval: false
dic_model_mcmc <- hddglm::dicModel(beta_chain,
                                  solution_path_mcmc,
                                  mean_mcmc,
                                  x[T,],
                                  y[T],
                                  T,
                                  burn_in = 500)
```

We evaluate the selected models by calling the function `variableSelectionTable` (see Table 4.4 in the thesis).

```{r}
#| eval: false

#evaluate the variables selected by AIC

#results based on MCMC algorithm 
hddglm::variableSelectionTable(aic_model_mcmc, 
                              relevant_param_75, 
                              p, 
                              T)
#results based on Linear Bayes method     
hddglm::variableSelectionTable(aic_model_lb, 
                              relevant_param_75, 
                              p, 
                              T)

#evaluate variables selected by DIC
hddglm::variableSelectionTable(dic_model_mcmc, 
                              relevant_param_75, 
                              p, 
                              T)
```
